{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to show  examples of functionnal submission packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General submission process\n",
    "\n",
    "All submissions are processed by the codabench plateform. In order to submit a model for the competition, the submission folder need to be compressed as a zip file (be carefull to compress all the files and not the folder itself, the unzipping need to recreate the file and not a folder containing the files). This zip can then be uploaded on the `my_submission` tab :\n",
    "\n",
    "![Alt text](utils/img/submission.png)\n",
    "\n",
    "Once submitted it is processed by the codabench plateform and send to one of our compute node for evaluation. It is possible to see the current status of the submission (submitted, waiting for worker, running, done) however the logs will only be available once the submission is done running. \n",
    "\n",
    "Please note that we currently have a 12hours limit for the execution of a submission (training, evaluation and scoring)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 : simple submission\n",
    "\n",
    "This example is available in submission/simple, a torch and tensorflow variations are provided.\n",
    "\n",
    "It correspond to a simple submission that use pre-implemented model and scaler and recreate the 1st example from the 4th notebook.\n",
    "This submission is composed of 3 files :\n",
    "- parameters.json\n",
    "- config.ini\n",
    "- scaler_parameters.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### parameters.json\n",
    "\n",
    "\n",
    "In this example we are using a fully connected model implemented through torch and already available in the LIPS package :\n",
    "- `from lips.augmented_simulators.torch_models.fully_connected import TorchFullyConnected`\n",
    "\n",
    "We also want to train and evaluate the model as such we indicate \n",
    "- `evaluateonly: false`\n",
    "- `scoringonly\": false`\n",
    "\n",
    "#### simulator_config :\n",
    "As we are use an already implemented simulator/model we use :\n",
    "- `simulator_type : simple_torch` \n",
    "Which indicate to the compute node that it will need to load the model from the LIPS package\n",
    "We name the model (used for saving an retrieving models, not important in this type of submission):\n",
    "- `name: \"MyAugmentedSimulator\"`\n",
    "And indicates to the compute node which model class and implementation we are using :\n",
    "- `model_type: \"fully_connected\"`\n",
    "- `model: \"TorchFullyConnected\"`\n",
    "\n",
    "This will load the following model when running :  `from lips.augmented_simulators.torch_models.fully_connected import TorchFullyConnected`\n",
    "\n",
    "In this example we also use a pre-implemented scaler : `from lips.dataset.scaler.standard_scaler import StandardScaler`\n",
    "Similarly we indicate which scaler class and implementation to load :\n",
    "- `scaler_type: \"simple\"`\n",
    "- `scaler_class: \"standard_scaler\"`\n",
    "- `scaler: \"StandardScaler\"`\n",
    "\n",
    "We then indicate which configuration will need to be used from the config.ini file (in this example we use the standard config presented in the 1st example of notebook 4):\n",
    "- `config_name: \"DEFAULT\"`\n",
    "\n",
    "#### simulator_extra_parameters:\n",
    "This section is used to pass custom parameters to the model and generally will be only used in association with a custom model as presented in the following example.\n",
    "As we are running a pre-implemented model, we do not pass any custom parameters and `simulator_extra_parameters` stay empty :\n",
    "- `simulator_extra_parameters: {}`\n",
    "\n",
    "#### training_config:\n",
    "We now configure to run the training for 10 epochs :\n",
    "- `training_config: {\"epoch\": 10}`\n",
    "\n",
    "Architecture_type is not used for the submission process and stay at \"Classical\"\n",
    "\n",
    "The resulting `parameters.json` file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"evaluateonly\": false,\n",
    "  \"scoringonly\": false,\n",
    "  \"simulator_config\": {\n",
    "    \"simulator_type\": \"simple_torch\",\n",
    "    \"name\": \"MyAugmentedSimulator\",\n",
    "    \"model\": \"TorchFullyConnected\",\n",
    "    \"model_type\": \"fully_connected\",\n",
    "    \"scaler_type\": \"simple\",\n",
    "    \"scaler_class\": \"standard_scaler\",\n",
    "    \"scaler\": \"StandardScaler\",\n",
    "    \"config_name\": \"DEFAULT\",\n",
    "  },\n",
    "  \"simulator_extra_parameters\": {},\n",
    "  \"training_config\": {\n",
    "    \"epochs\": 10\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config.ini\n",
    "This file is used to pass the configuration used in the model, as presented in notebook 4.\n",
    "We had the configuration file as defined in previous notebook :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[DEFAULT]\n",
    "name = \"torch_fc\"\n",
    "layers = (64,64,8,64,64,64,8,64,64)\n",
    "activation = \"relu\"\n",
    "layer = \"linear\"\n",
    "input_dropout = 0.0\n",
    "dropout = 0.0\n",
    "metrics = (\"MAELoss\",)\n",
    "loss = {\"name\": \"MSELoss\",\n",
    "        \"params\": {\"size_average\": None,\n",
    "                   \"reduce\": None,\n",
    "                   \"reduction\": 'mean'}}\n",
    "device = \"cpu\"\n",
    "optimizer = {\"name\": \"adam\",\n",
    "             \"params\": {\"lr\": 2e-4}}\n",
    "train_batch_size = 128000\n",
    "eval_batch_size = 256000\n",
    "epochs = 200\n",
    "shuffle = False\n",
    "save_freq = False\n",
    "ckpt_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaler_parameter.py\n",
    "\n",
    "This file contains the function that return the arguments for the scaler. The function take in argument the `benchmark` object (see notebook 4). In this example we use a standard iterative scaler implemented in LIPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a function that return the parameters of the scaler\n",
    "from lips.benchmark.airfransBenchmark import AirfRANSBenchmark\n",
    "\n",
    "def compute_scaler_parameters(benchmark):\n",
    "    chunk_sizes=benchmark.train_dataset.get_simulations_sizes()\n",
    "    no_norm_x=benchmark.train_dataset.get_no_normalization_axis_indices()\n",
    "    scalerParams={\"chunk_sizes\":chunk_sizes,\"no_norm_x\":no_norm_x}\n",
    "    return scalerParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 : custom torch model using LIPS simulator class\n",
    "\n",
    "This example is available in submission/custom_model\n",
    "\n",
    "It correspond to a submission that use a custom model implemented in `my_augmented_simulator.py` and a pre-implemented scaler. It recreates the 2nd example from the 4th notebook.\n",
    "This submission is composed of 4 files :\n",
    "- parameters.json\n",
    "- config.ini\n",
    "- scaler_parameters.py\n",
    "- my_augmented_simulator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters.json\n",
    "\n",
    "\n",
    "In this example we are using a custom model implemented in `my_augmented_simulator.py`\n",
    "\n",
    "We also want to train and evaluate the model as such we indicate \n",
    "- `evaluateonly: false`\n",
    "- `scoringonly\": false`\n",
    "\n",
    "#### simulator_config :\n",
    "As we are use a custom torch simulator we use :\n",
    "- `simulator_type : \"simple_torch\"`\n",
    "- `simulator_file : \"my_augmented_simulator\"`\n",
    "Which indicate to the compute node that it will need to load the model from `my_augmented_simulator.py`\n",
    "\n",
    "We name the model (used for saving an retrieving models, not important in this type of submission):\n",
    "- `name: \"MyAugmentedSimulator\"`\n",
    "And indicates to the compute node which model we are using :\n",
    "- `model: \"MyCustomFullyConnected\"`\n",
    "This correspond to the name of the class implemented in `my_augmented_simulator.py`.\n",
    "\n",
    "In this example we also use a pre-implemented scaler : `from lips.dataset.scaler.standard_scaler import StandardScaler`\n",
    "Similarly we indicate which scaler class and implementation to load :\n",
    "- `scaler_type: \"simple\"`\n",
    "- `scaler_class: \"standard_scaler\"`\n",
    "- `scaler: \"StandardScaler\"`\n",
    "\n",
    "We then indicate which configuration will need to be used from the config.ini file (in this example we use the standard config presented in the 1st example of notebook 4):\n",
    "- `config_name: \"DEFAULT\"`\n",
    "\n",
    "#### simulator_extra_parameters:\n",
    "This section is used to pass custom parameters to the model, it presents in the same form as training_config.\n",
    "In this case, we do not pass any custom parameters and `simulator_extra_parameters` stay empty :\n",
    "- `simulator_extra_parameters: {}`\n",
    "\n",
    "#### training_config:\n",
    "We now configure to run the training for 10 epochs :\n",
    "- `training_config: {\"epoch\": 10}`\n",
    "\n",
    "Architecture_type is not used for the submission process and stay at \"Classical\"\n",
    "\n",
    "The resulting `parameters.json` file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"evaluateonly\": false,\n",
    "  \"scoringonly\": false,\n",
    "  \"simulator_config\": {\n",
    "    \"simulator_type\": \"simple_torch\",\n",
    "    \"simulator_file\" : \"my_augmented_simulator\",\n",
    "    \"name\": \"MyAugmentedSimulator\",\n",
    "    \"model\": \"MyCustomFullyConnected\",\n",
    "    \"model_type\": \"my_augmented_simulator\",\n",
    "    \"scaler_type\": \"simple\",\n",
    "    \"scaler_class\": \"standard_scaler\",\n",
    "    \"scaler\": \"StandardScaler\",\n",
    "    \"config_name\": \"DEFAULT\",\n",
    "  },\n",
    "  \"simulator_extra_parameters\": {},\n",
    "  \"training_config\": {\n",
    "    \"epochs\": 10\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config.ini\n",
    "This file is used to pass the configuration used in the model, as presented in notebook 4.\n",
    "We had the configuration file as defined in previous notebook :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[DEFAULT]\n",
    "name = \"torch_fc\"\n",
    "layers = (64,64,8,64,64,64,8,64,64)\n",
    "activation = \"relu\"\n",
    "layer = \"linear\"\n",
    "input_dropout = 0.0\n",
    "dropout = 0.0\n",
    "metrics = (\"MAELoss\",)\n",
    "loss = {\"name\": \"MSELoss\",\n",
    "        \"params\": {\"size_average\": None,\n",
    "                   \"reduce\": None,\n",
    "                   \"reduction\": 'mean'}}\n",
    "device = \"cpu\"\n",
    "optimizer = {\"name\": \"adam\",\n",
    "             \"params\": {\"lr\": 2e-4}}\n",
    "train_batch_size = 128000\n",
    "eval_batch_size = 256000\n",
    "epochs = 200\n",
    "shuffle = False\n",
    "save_freq = False\n",
    "ckpt_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaler_parameter.py\n",
    "\n",
    "This file contains the function that return the arguments for the scaler. The function take in argument the `benchmark` object (see notebook 4). In this example we use a standard iterative scaler implemented in LIPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a function that return the parameters of the scaler\n",
    "from lips.benchmark.airfransBenchmark import AirfRANSBenchmark\n",
    "\n",
    "def compute_scaler_parameters(benchmark):\n",
    "    chunk_sizes=benchmark.train_dataset.get_simulations_sizes()\n",
    "    no_norm_x=benchmark.train_dataset.get_no_normalization_axis_indices()\n",
    "    scalerParams={\"chunk_sizes\":chunk_sizes,\"no_norm_x\":no_norm_x}\n",
    "    return scalerParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my_augmented_simulator.py\n",
    "\n",
    "This file contains the implementation of a custom model. This implementation needs to be compatible with the LIPS simulator class in order for the simulation and evaluation processes to be able to access it. Here we implement and example of a fully connected pytorch model as seen in the 2nd example of the 4th notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Torch fully connected model\n",
    "\"\"\"\n",
    "import os\n",
    "import pathlib\n",
    "from typing import Union\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from lips.dataset import DataSet\n",
    "from lips.dataset.scaler import Scaler\n",
    "from lips.logger import CustomLogger\n",
    "from lips.config import ConfigManager\n",
    "from lips.utils import NpEncoder\n",
    "\n",
    "class MyCustomFullyConnected(nn.Module):\n",
    "    def __init__(self,\n",
    "                 sim_config_path: Union[pathlib.Path, str],\n",
    "                 bench_config_path: Union[str, pathlib.Path],\n",
    "                 sim_config_name: Union[str, None]=None,\n",
    "                 bench_config_name: Union[str, None]=None,\n",
    "                 name: Union[str, None]=None,\n",
    "                 scaler: Union[Scaler, None]=None,\n",
    "                 log_path: Union[None, pathlib.Path, str]=None,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        if not os.path.exists(sim_config_path):\n",
    "            raise RuntimeError(\"Configuration path for the simulator not found!\")\n",
    "        if not str(sim_config_path).endswith(\".ini\"):\n",
    "            raise RuntimeError(\"The configuration file should have `.ini` extension!\")\n",
    "        # if test_custom_param:\n",
    "        #     print(\"test_custom_param: \", test_custom_param)\n",
    "        sim_config_name = sim_config_name if sim_config_name is not None else \"DEFAULT\"\n",
    "        self.sim_config = ConfigManager(section_name=sim_config_name, path=sim_config_path)\n",
    "        self.bench_config = ConfigManager(section_name=bench_config_name, path=bench_config_path)\n",
    "        self.name = name if name is not None else self.sim_config.get_option(\"name\")\n",
    "        # scaler\n",
    "        self.scaler = scaler\n",
    "        # Logger\n",
    "        self.log_path = log_path\n",
    "        self.logger = CustomLogger(__class__.__name__, log_path).logger\n",
    "        # model parameters\n",
    "        self.params = self.sim_config.get_options_dict()\n",
    "        self.params.update(kwargs)\n",
    "\n",
    "        self.activation = {\n",
    "            \"relu\": F.relu,\n",
    "            \"sigmoid\": F.sigmoid,\n",
    "            \"tanh\": F.tanh\n",
    "        }\n",
    "\n",
    "        self.input_size = None if kwargs.get(\"input_size\") is None else kwargs[\"input_size\"]\n",
    "        self.output_size = None if kwargs.get(\"output_size\") is None else kwargs[\"output_size\"]\n",
    "\n",
    "        self.input_layer = None\n",
    "        self.input_dropout = None\n",
    "        self.fc_layers = None\n",
    "        self.dropout_layers = None\n",
    "        self.output_layer = None\n",
    "\n",
    "        #self.__build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build the model architecture\n",
    "        \"\"\"\n",
    "        linear_sizes = list(self.params[\"layers\"])\n",
    "\n",
    "        self.input_layer = nn.Linear(self.input_size, linear_sizes[0])\n",
    "        self.input_dropout = nn.Dropout(p=self.params[\"input_dropout\"])\n",
    "\n",
    "        self.fc_layers = nn.ModuleList([nn.Linear(in_f, out_f) \\\n",
    "            for in_f, out_f in zip(linear_sizes[:-1], linear_sizes[1:])])\n",
    "\n",
    "        self.dropout_layers = nn.ModuleList([nn.Dropout(p=self.params[\"dropout\"]) \\\n",
    "            for _ in range(len(self.fc_layers))])\n",
    "\n",
    "        self.output_layer = nn.Linear(linear_sizes[-1], self.output_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"The forward pass of the model\n",
    "        \"\"\"\n",
    "        out = self.input_layer(data)\n",
    "        out = self.input_dropout(out)\n",
    "        for _, (fc_, dropout) in enumerate(zip(self.fc_layers, self.dropout_layers)):\n",
    "            out = fc_(out)\n",
    "            out = self.activation[self.params[\"activation\"]](out)\n",
    "            out = dropout(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "\n",
    "    def process_dataset(self, dataset: DataSet, training: bool):\n",
    "        \"\"\"process the datasets for training and evaluation\n",
    "\n",
    "        This function transforms all the dataset into something that can be used by the neural network (for example)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : DataSet\n",
    "            A dataset that should be processed\n",
    "        training : bool, optional\n",
    "            indicate if we are in training phase or not, by default False\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataLoader\n",
    "            _description_\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            self._infer_size(dataset)\n",
    "            batch_size = self.params[\"train_batch_size\"]\n",
    "            extract_x, extract_y = dataset.extract_data()\n",
    "            if self.scaler is not None:\n",
    "                extract_x, extract_y = self.scaler.fit_transform(extract_x, extract_y)\n",
    "        else:\n",
    "            batch_size = self.params[\"eval_batch_size\"]\n",
    "            extract_x, extract_y = dataset.extract_data()\n",
    "            if self.scaler is not None:\n",
    "                extract_x, extract_y = self.scaler.transform(extract_x, extract_y)\n",
    "\n",
    "        torch_dataset = TensorDataset(torch.from_numpy(extract_x).float(), torch.from_numpy(extract_y).float())\n",
    "        data_loader = DataLoader(torch_dataset, batch_size=batch_size, shuffle=self.params[\"shuffle\"])\n",
    "        return data_loader\n",
    "\n",
    "    def _post_process(self, data):\n",
    "        \"\"\"\n",
    "        This function is used to inverse the predictions of the model to their original state, before scaling\n",
    "        to be able to compare them with ground truth data\n",
    "        \"\"\"\n",
    "        if self.scaler is not None:\n",
    "            try:\n",
    "                processed = self.scaler.inverse_transform(data)\n",
    "            except TypeError:\n",
    "                processed = self.scaler.inverse_transform(data.cpu())\n",
    "        else:\n",
    "            processed = data\n",
    "        return processed\n",
    "\n",
    "    def _infer_size(self, dataset: DataSet):\n",
    "        \"\"\"Infer the size of the input and ouput variables\n",
    "        \"\"\"\n",
    "        *dim_inputs, self.output_size = dataset.get_sizes()\n",
    "        self.input_size = np.sum(dim_inputs)\n",
    "\n",
    "    def get_metadata(self):\n",
    "        res_json = {}\n",
    "        res_json[\"input_size\"] = self.input_size\n",
    "        res_json[\"output_size\"] = self.output_size\n",
    "        return res_json\n",
    "\n",
    "    def _save_metadata(self, path: str):\n",
    "        res_json = {}\n",
    "        res_json[\"input_size\"] = self.input_size\n",
    "        res_json[\"output_size\"] = self.output_size\n",
    "        with open((path / \"metadata.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(obj=res_json, fp=f, indent=4, sort_keys=True, cls=NpEncoder)\n",
    "\n",
    "    def _load_metadata(self, path: str):\n",
    "        if not isinstance(path, pathlib.Path):\n",
    "            path = pathlib.Path(path)\n",
    "        with open((path / \"metadata.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            res_json = json.load(fp=f)\n",
    "        self.input_size = res_json[\"input_size\"]\n",
    "        self.output_size = res_json[\"output_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3 : fully custom model independent from the LIPS framework\n",
    "\n",
    "This example is available in submission/fully_custom_model\n",
    "\n",
    "It correspond to a submission that use a custom model implemented in `my_augmented_simulator.py`. It recreates the 3b notebook.\n",
    "This submission is composed of 2 files  :\n",
    "- parameters.json\n",
    "- my_augmented_simulator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parameters.json\n",
    "\n",
    "\n",
    "In this example we are using a custom model implemented in `my_augmented_simulator.py`\n",
    "\n",
    "We also want to train and evaluate the model as such we indicate \n",
    "- `evaluateonly: false`\n",
    "- `scoringonly\": false`\n",
    "\n",
    "#### simulator_config :\n",
    "As we are use a custom simulator we use :\n",
    "- `simulator_type : \"custom\"`\n",
    "- `simulator_file : \"my_augmented_simulator\"`\n",
    "Which indicate to the compute node that it will need to load the model from `my_augmented_simulator.py`\n",
    "\n",
    "We name the model (used for saving an retrieving models, not important in this type of submission):\n",
    "- `name: \"MyAugmentedSimulator\"`\n",
    "And indicates to the compute node which model we are using :\n",
    "- `model: \"MyCustomFullyConnected\"`\n",
    "This correspond to the name of the class implemented in `my_augmented_simulator.py`.\n",
    "\n",
    "In this type of submission all data treatment including scalers need to be implemented in the model, we therefore use :\n",
    "- `scaler_type`: \"None\"\n",
    "\n",
    "#### simulator_extra_parameters:\n",
    "This section is used to pass custom parameters to the model, it presents in the same form as training_config. For this example we use the following:\n",
    "- `simulator_extra_parameters: {    \n",
    "    \"encoder\": [7, 64, 64, 8],\n",
    "    \"decoder\": [8, 64, 64, 4],\n",
    "    \"nb_hidden_layers\": 3,\n",
    "    \"size_hidden_layers\": 64,\n",
    "    \"batch_size\": 1,\n",
    "    \"nb_epochs\": 600,\n",
    "    \"lr\": 0.001,\n",
    "    \"bn_bool\": true,\n",
    "    \"subsampling\": 32000}`\n",
    "\n",
    "#### training_config:\n",
    "We do not use special parameters in the training (they are passed directly to the simulator in this implementation):\n",
    "- `training_config: {}`\n",
    "\n",
    "The resulting `parameters.json` file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"evaluateonly\": false,\n",
    "  \"scoringonly\": false,\n",
    "  \"simulator_config\": {\n",
    "    \"simulator_type\": \"custom\",\n",
    "    \"simulator_file\": \"my_augmented_simulator\",\n",
    "    \"name\": \"MyAugmentedSimulator\",\n",
    "    \"model\": \"AugmentedSimulator\",\n",
    "    \"scaler_type\": \"None\"\n",
    "   },\n",
    "  \"simulator_extra_parameters\": {\n",
    "    \"encoder\": [7, 64, 64, 8],\n",
    "    \"decoder\": [8, 64, 64, 4],\n",
    "    \"nb_hidden_layers\": 3,\n",
    "    \"size_hidden_layers\": 64,\n",
    "    \"batch_size\": 1,\n",
    "    \"nb_epochs\": 600,\n",
    "    \"lr\": 0.001,\n",
    "    \"bn_bool\": true,\n",
    "    \"subsampling\": 32000\n",
    "  },\n",
    "  \"training_config\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my_augmented_simulator.py\n",
    "\n",
    "This file contains the implementation of a custom model. The corresponding class need to be runnable by the ingestion process and as such needs the following functions :\n",
    "- __init__(self,benchmark,**kwargs)\n",
    "- train(self,train_dataset, save_path=None)\n",
    "- predict(self,dataset,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math \n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import BatchNorm1d, Identity\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from lips import get_root_path\n",
    "from lips.benchmark.airfransBenchmark import AirfRANSBenchmark\n",
    "from lips.dataset.airfransDataSet import download_data\n",
    "from lips.dataset.scaler.standard_scaler_iterative import StandardScalerIterative\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, channel_list, dropout = 0.,\n",
    "                 batch_norm = True, relu_first = False):\n",
    "        super().__init__()\n",
    "        assert len(channel_list) >= 2\n",
    "        self.channel_list = channel_list\n",
    "        self.dropout = dropout\n",
    "        self.relu_first = relu_first\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        for dims in zip(self.channel_list[:-1], self.channel_list[1:]):\n",
    "            self.lins.append(Linear(*dims))\n",
    "\n",
    "        self.norms = torch.nn.ModuleList()\n",
    "        for dim in zip(self.channel_list[1:-1]):\n",
    "            self.norms.append(BatchNorm1d(dim, track_running_stats = False) if batch_norm else Identity())\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        for norm in self.norms:\n",
    "            if hasattr(norm, 'reset_parameters'):\n",
    "                norm.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        x = self.lins[0](x)\n",
    "        for lin, norm in zip(self.lins[1:], self.norms):\n",
    "            if self.relu_first:\n",
    "                x = x.relu_()\n",
    "            x = norm(x)\n",
    "            if not self.relu_first:\n",
    "                x = x.relu_()\n",
    "            x = F.dropout(x, p = self.dropout, training = self.training)\n",
    "            x = lin.forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}({str(self.channel_list)[1:-1]})'\n",
    "\n",
    "\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, hparams, encoder, decoder):\n",
    "        super(NN, self).__init__()\n",
    "        self.nb_hidden_layers = hparams['nb_hidden_layers']\n",
    "        self.size_hidden_layers = hparams['size_hidden_layers']\n",
    "        self.bn_bool = hparams['bn_bool']\n",
    "        self.activation = nn.ReLU()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.dim_enc = hparams['encoder'][-1]\n",
    "        self.nn = MLP([self.dim_enc] + [self.size_hidden_layers]*self.nb_hidden_layers + [self.dim_enc], batch_norm = self.bn_bool)\n",
    "\n",
    "    def forward(self, data):\n",
    "        z = self.encoder(data.x)        \n",
    "        z = self.nn(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "class AugmentedSimulator():\n",
    "    def __init__(self,benchmark,**kwargs):\n",
    "        self.name = \"AirfRANSSubmission\"\n",
    "        chunk_sizes=benchmark.train_dataset.get_simulations_sizes()\n",
    "        scalerParams={\"chunk_sizes\":chunk_sizes}\n",
    "        self.scaler = StandardScalerIterative(**scalerParams)\n",
    "\n",
    "        self.model = None\n",
    "        self.hparams = kwargs\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        self.device = 'cuda:0' if use_cuda else 'cpu'\n",
    "        if use_cuda:\n",
    "            print('Using GPU')\n",
    "        else:\n",
    "            print('Using CPU')\n",
    "\n",
    "        encoder = MLP(self.hparams['encoder'], batch_norm = False)\n",
    "        decoder = MLP(self.hparams['decoder'], batch_norm = False)\n",
    "        self.model = NN(self.hparams, encoder, decoder)\n",
    "\n",
    "    def process_dataset(self, dataset, training: bool) -> DataLoader:\n",
    "        coord_x=dataset.data['x-position']\n",
    "        coord_y=dataset.data['y-position']\n",
    "        surf_bool=dataset.extra_data['surface']\n",
    "        position = np.stack([coord_x,coord_y],axis=1)\n",
    "\n",
    "        nodes_features,node_labels=dataset.extract_data()\n",
    "        if training:\n",
    "            print(\"Normalize train data\")\n",
    "            nodes_features, node_labels = self.scaler.fit_transform(nodes_features, node_labels)\n",
    "        else:\n",
    "            print(\"Normalize not train data\")\n",
    "            nodes_features, node_labels = self.scaler.transform(nodes_features, node_labels)\n",
    "\n",
    "        torchDataset=[]\n",
    "        nb_nodes_in_simulations = dataset.get_simulations_sizes()\n",
    "        start_index = 0\n",
    "        for nb_nodes_in_simulation in nb_nodes_in_simulations:\n",
    "            end_index = start_index+nb_nodes_in_simulation\n",
    "            simulation_positions = torch.tensor(position[start_index:end_index,:], dtype = torch.float) \n",
    "            simulation_features = torch.tensor(nodes_features[start_index:end_index,:], dtype = torch.float) \n",
    "            simulation_labels = torch.tensor(node_labels[start_index:end_index,:], dtype = torch.float) \n",
    "            simulation_surface = torch.tensor(surf_bool[start_index:end_index])\n",
    "\n",
    "            sampleData=Data(pos=simulation_positions,\n",
    "                            x=simulation_features, \n",
    "                            y=simulation_labels,\n",
    "                            surf = simulation_surface.bool()) \n",
    "            torchDataset.append(sampleData)\n",
    "            start_index += nb_nodes_in_simulation\n",
    "        return DataLoader(dataset=torchDataset,batch_size=1)\n",
    "\n",
    "    def train(self,train_dataset, save_path=None):\n",
    "        train_dataset = self.process_dataset(dataset=train_dataset,training=True)\n",
    "        model = global_train(self.device, train_dataset, self.model, self.hparams,criterion = 'MSE_weighted')\n",
    "\n",
    "    def predict(self,dataset,**kwargs):\n",
    "        print(dataset)\n",
    "        test_dataset = self.process_dataset(dataset=dataset,training=False)\n",
    "        self.model.eval()\n",
    "        avg_loss_per_var = np.zeros(4)\n",
    "        avg_loss = 0\n",
    "        avg_loss_surf_var = np.zeros(4)\n",
    "        avg_loss_vol_var = np.zeros(4)\n",
    "        avg_loss_surf = 0\n",
    "        avg_loss_vol = 0\n",
    "        iterNum = 0\n",
    "\n",
    "        predictions=[]\n",
    "        with torch.no_grad():\n",
    "            for data in test_dataset:        \n",
    "                data_clone = data.clone()\n",
    "                data_clone = data_clone.to(self.device)\n",
    "                out = self.model(data_clone)\n",
    "\n",
    "                targets = data_clone.y\n",
    "                loss_criterion = nn.MSELoss(reduction = 'none')\n",
    "\n",
    "                loss_per_var = loss_criterion(out, targets).mean(dim = 0)\n",
    "                loss = loss_per_var.mean()\n",
    "                loss_surf_var = loss_criterion(out[data_clone.surf, :], targets[data_clone.surf, :]).mean(dim = 0)\n",
    "                loss_vol_var = loss_criterion(out[~data_clone.surf, :], targets[~data_clone.surf, :]).mean(dim = 0)\n",
    "                loss_surf = loss_surf_var.mean()\n",
    "                loss_vol = loss_vol_var.mean()  \n",
    "\n",
    "                avg_loss_per_var += loss_per_var.cpu().numpy()\n",
    "                avg_loss += loss.cpu().numpy()\n",
    "                avg_loss_surf_var += loss_surf_var.cpu().numpy()\n",
    "                avg_loss_vol_var += loss_vol_var.cpu().numpy()\n",
    "                avg_loss_surf += loss_surf.cpu().numpy()\n",
    "                avg_loss_vol += loss_vol.cpu().numpy()  \n",
    "                iterNum += 1\n",
    "\n",
    "                out = out.cpu().data.numpy()\n",
    "                prediction = self._post_process(out)\n",
    "                predictions.append(prediction)\n",
    "        print(\"Results for test\")\n",
    "        print(avg_loss/iterNum, avg_loss_per_var/iterNum, avg_loss_surf_var/iterNum, avg_loss_vol_var/iterNum, avg_loss_surf/iterNum, avg_loss_vol/iterNum)\n",
    "        predictions= np.vstack(predictions)\n",
    "        predictions = dataset.reconstruct_output(predictions)\n",
    "        return predictions\n",
    "\n",
    "    def _post_process(self, data):\n",
    "        try:\n",
    "            processed = self.scaler.inverse_transform(data)\n",
    "        except TypeError:\n",
    "            processed = self.scaler.inverse_transform(data.cpu())\n",
    "        return processed\n",
    "\n",
    "\n",
    "def global_train(device, train_dataset, network, hparams, criterion = 'MSE', reg = 1):\n",
    "    model = network.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = hparams['lr'])\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr = hparams['lr'],\n",
    "            total_steps = (len(train_dataset) // hparams['batch_size'] + 1) * hparams['nb_epochs'],\n",
    "        )\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss_surf_list = []\n",
    "    train_loss_vol_list = []\n",
    "    loss_surf_var_list = []\n",
    "    loss_vol_var_list = []\n",
    "\n",
    "    pbar_train = tqdm(range(hparams['nb_epochs']), position=0)\n",
    "    for epoch in pbar_train:        \n",
    "        train_dataset_sampled = []\n",
    "        for data in train_dataset:\n",
    "            data_sampled = data.clone()\n",
    "            idx = random.sample(range(data_sampled.x.size(0)), hparams['subsampling'])\n",
    "            idx = torch.tensor(idx)\n",
    "\n",
    "            data_sampled.pos = data_sampled.pos[idx]\n",
    "            data_sampled.x = data_sampled.x[idx]\n",
    "            data_sampled.y = data_sampled.y[idx]\n",
    "            data_sampled.surf = data_sampled.surf[idx]\n",
    "            train_dataset_sampled.append(data_sampled)\n",
    "        train_loader = DataLoader(train_dataset_sampled, batch_size = hparams['batch_size'], shuffle = True)\n",
    "        del(train_dataset_sampled)\n",
    "\n",
    "        train_loss, _, loss_surf_var, loss_vol_var, loss_surf, loss_vol = train_model(device, model, train_loader, optimizer, lr_scheduler, criterion, reg = reg)        \n",
    "        if criterion == 'MSE_weighted':\n",
    "            train_loss = reg*loss_surf + loss_vol\n",
    "        del(train_loader)\n",
    "\n",
    "        train_loss_surf_list.append(loss_surf)\n",
    "        train_loss_vol_list.append(loss_vol)\n",
    "        loss_surf_var_list.append(loss_surf_var)\n",
    "        loss_vol_var_list.append(loss_vol_var)\n",
    "\n",
    "    loss_surf_var_list = np.array(loss_surf_var_list)\n",
    "    loss_vol_var_list = np.array(loss_vol_var_list)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(device, model, train_loader, optimizer, scheduler, criterion = 'MSE', reg = 1):\n",
    "    model.train()\n",
    "    avg_loss_per_var = torch.zeros(4, device = device)\n",
    "    avg_loss = 0\n",
    "    avg_loss_surf_var = torch.zeros(4, device = device)\n",
    "    avg_loss_vol_var = torch.zeros(4, device = device)\n",
    "    avg_loss_surf = 0\n",
    "    avg_loss_vol = 0\n",
    "    iterNum = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        data_clone = data.clone()\n",
    "        data_clone = data_clone.to(device)   \n",
    "        optimizer.zero_grad()  \n",
    "        out = model(data_clone)\n",
    "        targets = data_clone.y\n",
    "\n",
    "        if criterion == 'MSE' or criterion == 'MSE_weighted':\n",
    "            loss_criterion = nn.MSELoss(reduction = 'none')\n",
    "        elif criterion == 'MAE':\n",
    "            loss_criterion = nn.L1Loss(reduction = 'none')\n",
    "        loss_per_var = loss_criterion(out, targets).mean(dim = 0)\n",
    "        total_loss = loss_per_var.mean()\n",
    "        loss_surf_var = loss_criterion(out[data_clone.surf, :], targets[data_clone.surf, :]).mean(dim = 0)\n",
    "        loss_vol_var = loss_criterion(out[~data_clone.surf, :], targets[~data_clone.surf, :]).mean(dim = 0)\n",
    "        loss_surf = loss_surf_var.mean()\n",
    "        loss_vol = loss_vol_var.mean()\n",
    "\n",
    "        if criterion == 'MSE_weighted':            \n",
    "            (loss_vol + reg*loss_surf).backward()           \n",
    "        else:\n",
    "            total_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        avg_loss_per_var += loss_per_var\n",
    "        avg_loss += total_loss\n",
    "        avg_loss_surf_var += loss_surf_var\n",
    "        avg_loss_vol_var += loss_vol_var\n",
    "        avg_loss_surf += loss_surf\n",
    "        avg_loss_vol += loss_vol \n",
    "        iterNum += 1\n",
    "\n",
    "    return avg_loss.cpu().data.numpy()/iterNum, avg_loss_per_var.cpu().data.numpy()/iterNum, avg_loss_surf_var.cpu().data.numpy()/iterNum, avg_loss_vol_var.cpu().data.numpy()/iterNum, \\\n",
    "            avg_loss_surf.cpu().data.numpy()/iterNum, avg_loss_vol.cpu().data.numpy()/iterNum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 4 : load trained model\n",
    "\n",
    "This example is available in submission/load_trained_model\n",
    "\n",
    "**Note : This type of submission is only for informative purpose, in order for a submission to be valid for the final ranking it needs to be trained and evaluated by the compute node.**\n",
    "We offer the possibility to load a trained model in order to evaluate and score it on the compute node. This can be useful to test the submission while limiting the use of compute power on the competition part.\n",
    "\n",
    "In this example we use the same custom model as the previous example.It recreates the 3b notebook with a pre-trained model.\n",
    "This submission is composed of 2 files and a folder containing the pre-trained model :\n",
    "- parameters.json\n",
    "- my_augmented_simulator.py\n",
    "- trained_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parameters.json\n",
    "\n",
    "As we are using the same model as example 3 we only need to change the evaluateonly parameter :\n",
    "- `evaluateonly: true`\n",
    "- `scoringonly\": false`\n",
    "\n",
    "The rest of the parameters stay the same and correspond to the parameters needed for the model being loaded:\n",
    "#### simulator_config :\n",
    "As we are use a custom simulator we use :\n",
    "- `simulator_type : \"custom\"`\n",
    "- `simulator_file : \"my_augmented_simulator\"`\n",
    "Which indicate to the compute node that it will need to load the model from `my_augmented_simulator.py`\n",
    "\n",
    "We name the model (used for saving an retrieving models, not important in this type of submission):\n",
    "- `name: \"MyAugmentedSimulator\"`\n",
    "And indicates to the compute node which model we are using :\n",
    "- `model: \"MyCustomFullyConnected\"`\n",
    "This correspond to the name of the class implemented in `my_augmented_simulator.py`.\n",
    "\n",
    "In this type of submission all data treatment including scalers need to be implemented in the model, we therefore use :\n",
    "- `scaler_type`: \"None\"\n",
    "\n",
    "#### simulator_extra_parameters:\n",
    "This section is used to pass custom parameters to the model, it presents in the same form as training_config. For this example we use the following:\n",
    "- `simulator_extra_parameters: {    \n",
    "    \"encoder\": [7, 64, 64, 8],\n",
    "    \"decoder\": [8, 64, 64, 4],\n",
    "    \"nb_hidden_layers\": 3,\n",
    "    \"size_hidden_layers\": 64,\n",
    "    \"batch_size\": 1,\n",
    "    \"nb_epochs\": 600,\n",
    "    \"lr\": 0.001,\n",
    "    \"bn_bool\": true,\n",
    "    \"subsampling\": 32000}`\n",
    "\n",
    "#### training_config:\n",
    "We do not use special parameters in the training (they are passed directly to the simulator in this implementation):\n",
    "- `training_config: {}`\n",
    "\n",
    "The resulting `parameters.json` file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"evaluateonly\": false,\n",
    "  \"scoringonly\": false,\n",
    "  \"simulator_config\": {\n",
    "    \"simulator_type\": \"custom\",\n",
    "    \"simulator_file\": \"my_augmented_simulator\",\n",
    "    \"name\": \"MyAugmentedSimulator\",\n",
    "    \"model\": \"AugmentedSimulator\",\n",
    "    \"scaler_type\": \"None\"\n",
    "   },\n",
    "  \"simulator_extra_parameters\": {\n",
    "    \"encoder\": [7, 64, 64, 8],\n",
    "    \"decoder\": [8, 64, 64, 4],\n",
    "    \"nb_hidden_layers\": 3,\n",
    "    \"size_hidden_layers\": 64,\n",
    "    \"batch_size\": 1,\n",
    "    \"nb_epochs\": 600,\n",
    "    \"lr\": 0.001,\n",
    "    \"bn_bool\": true,\n",
    "    \"subsampling\": 32000\n",
    "  },\n",
    "  \"training_config\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be saved and loaded the simulator need to also implement the following function which is called while running the ingestion :\n",
    "- restore(self, path:str) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following functions are added to the simulator class in order to load the model and the scaler:\n",
    "    def restore(self, path):\n",
    "        self.load_model(path_model=os.path.join(path, 'SaveFCModel.pt'), path_scaler=os.path.join(path, 'SaveScaler'))\n",
    "\n",
    "    def save_model(self, path_model:str,path_scaler:str):\n",
    "        modelWeight=self.model.state_dict()\n",
    "        torch.save(modelWeight,path_model)\n",
    "        self.scaler.save(path_scaler)\n",
    "\n",
    "    def load_model(self, path_model:str,path_scaler:str):\n",
    "        model_loader=torch.load(path_model)\n",
    "        self.model.load_state_dict(model_loader)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.scaler.load(path_scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
