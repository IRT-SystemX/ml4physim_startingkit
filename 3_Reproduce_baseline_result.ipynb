{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Baselines (Airfoil use case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to demonstrate how we can evaluate the results of a baseline on a given benchmark.\n",
    "\n",
    "We will show how to load a baseline (or any other `AugmentedSimulator`) and evaluate it on a `Benchmark` of our choice.\n",
    "\n",
    "**To learn more about the training procedure, visit [this notebook](https://github.com/IRT-SystemX/LIPS/blob/main/getting_started/PowerGridUsecase/03_TrainAnAugmentedSimulator.ipynb)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the LIPS framework if it is not already done. For more information look at the LIPS framework [Github repository](https://github.com/IRT-SystemX/LIPS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# or \n",
    "# !pip install -U ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the AirfRANS package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install airfrans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lips import get_root_path\n",
    "from lips.dataset.airfransDataSet import download_data\n",
    "from lips.benchmark.airfransBenchmark import AirfRANSBenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate required paths\n",
    "LIPS_PATH = get_root_path()\n",
    "DIRECTORY_NAME = 'Dataset'\n",
    "BENCHMARK_NAME = \"Case1\"\n",
    "LOG_PATH = LIPS_PATH + \"lips_logs.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the configuration files path, that aim to describe specific caracteristics of the use case or the augmented simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCH_CONFIG_PATH = os.path.join(\"airfoilConfigurations\",\"benchmarks\",\"confAirfoil.ini\") #Configuration file related to the benchmark\n",
    "SIM_CONFIG_PATH = os.path.join(\"airfoilConfigurations\",\"simulators\",\"torch_fc.ini\") #Configuration file re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial step: download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isdir(DIRECTORY_NAME):\n",
    "    download_data(root_path=\".\", directory_name=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Benchmark <a id=\"Case1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step: load the dataset\n",
    "\n",
    "A common dataset will be used for evaluate the augmented simulator. This initial step aims at loading it once and for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark=AirfRANSBenchmark(benchmark_path = DIRECTORY_NAME,\n",
    "                            config_path = BENCH_CONFIG_PATH,\n",
    "                            benchmark_name = BENCHMARK_NAME,\n",
    "                            log_path = LOG_PATH)\n",
    "benchmark.load(path=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the config is loaded appropriately for this benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark name: \", benchmark.config.section_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each simulation is given as a point cloud defined via the nodes of the simulation mesh. Each point of a point cloud is described via 7 features:\n",
    "- its position (two component in meter): 'x-position' and 'y-position'\n",
    "- the inlet velocity (two components in meter per second): 'x-inlet_velocity' and 'y-inlet_velocity'\n",
    "- the distance to the airfoil (one component in meter): distance_function\n",
    "- the normals (two components in meter, set to 0 if the point is not on the airfoil): 'x-normals', 'y-normals'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input attributes (features): \", benchmark.config.get_option(\"attr_x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each point is given a target of 4 components for the underlying regression task:\n",
    "- the velocity (two components in meter per second): 'x-velocity'and 'y-velocity'\n",
    "- the pressure divided by the specific mass (one component in meter squared per second squared): 'pressure'\n",
    "- the turbulent kinematic viscosity (one component in meter squared per second): 'turbulent_viscosity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output attributes (targets): \", benchmark.config.get_option(\"attr_y\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation criteria can be found in the configure file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation criteria: \")\n",
    "criteria = benchmark.config.get_option(\"eval_dict\")\n",
    "for criteriaCategory,criteriaNames in criteria.items():\n",
    "    print(\"\\t Category %s: \" %criteriaCategory,\" \",criteriaNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the machine learning metrics are relatively standard, the physical metrics are closery related to the underlying use case and physical problem. We provide a description of each of them. \n",
    "\n",
    "There are two physical quantities considered\n",
    "- the drag coefficient\n",
    "- the lift coefficient\n",
    "\n",
    "For each of them and for each dataset, we compute between the observations and predictions coefficients:\n",
    "- the spearman correlation, a nonparametric measure of the monotonicity of the relationship between two datasets: 'spearman_correlation_drag', 'spearman_correlation_lift'\n",
    "- the mean relative error: 'mean_relative_drag', 'mean_relative_lift'\n",
    "- the standard deviation: 'std_relative_drag', 'std_relative_lift'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have a look at the datasets (namely features, label, size of each variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train dataset: \", benchmark.train_dataset)\n",
    "print(\"test dataset: \", benchmark._test_dataset )\n",
    "print(\"test dataset: \", benchmark._test_ood_dataset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A baseline \"augmented simulator\" <a id=\"bench1-fc\"></a>\n",
    "\n",
    "Along with some dataset, we provide also some baseline (from a trained neural network). This baseline is made of a fully connected neural network that takes the available input of the airfrans case and tries to predict all the output of the simulator.\n",
    "\n",
    "The fully connected neural network is made of XXX layer each with YYY units.\n",
    "\n",
    "It is learned for KKK epochs on the training set of the `Case1`.\n",
    "\n",
    "First we need to load the baseline and initialize it properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.augmented_simulators.torch_models.fully_connected import TorchFullyConnected\n",
    "from lips.augmented_simulators.torch_simulator import TorchSimulator\n",
    "from lips.dataset.scaler.standard_scaler_iterative import StandardScalerIterative\n",
    "\n",
    "\n",
    "chunk_sizes=benchmark.train_dataset.get_simulations_sizes()\n",
    "no_norm_x=benchmark.train_dataset.get_no_normalization_axis_indices()\n",
    "scalerParams={\"chunk_sizes\":chunk_sizes,\"no_norm_x\":no_norm_x}\n",
    "\n",
    "augmented_simulator = TorchSimulator(name=\"torch_fc\",\n",
    "                                     model=TorchFullyConnected,\n",
    "                                     scaler=StandardScalerIterative,\n",
    "                                     log_path=\"log_benchmark\",\n",
    "                                     device=\"cuda:0\",\n",
    "                                     bench_config_path=BENCH_CONFIG_PATH,\n",
    "                                     bench_config_name=BENCHMARK_NAME,\n",
    "                                     sim_config_path=SIM_CONFIG_PATH,\n",
    "                                     sim_config_name=\"DEFAULT\",\n",
    "                                     architecture_type=\"Classical\",\n",
    "                                     scalerParams=scalerParams\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_simulator.train(train_dataset=benchmark.train_dataset, epochs=2, train_batch_size=128000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate it on the test datasets of the benchmark. This is done by indicating the learned augmented simulator `augmented_simulator` as the argument of the `evaluate_simulator` method.\n",
    "\n",
    "There are 2 steps performed within the evaluation of a simulator:\n",
    "- Compute the prediction using the augmented simulator\n",
    "- Evaluate the performances of the datasets within the benchmark with respect to the observation based on the prescribed metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_test = time.time()\n",
    "fc_metrics_test = benchmark.evaluate_simulator(dataset=\"test\",augmented_simulator=augmented_simulator,eval_batch_size=256000 )\n",
    "test_evaluation_time = time.time() - start_test\n",
    "test_mean_simulation_time = test_evaluation_time/len(benchmark._test_dataset.get_simulations_sizes())\n",
    "\n",
    "start_test_ood = time.time()\n",
    "fc_metrics_test_ood = benchmark.evaluate_simulator(dataset=\"test_ood\",augmented_simulator=augmented_simulator,eval_batch_size=256000 )\n",
    "test_ood_evaluation_time = time.time() - start_test_ood\n",
    "test_ood_mean_simulation_time = test_ood_evaluation_time/len(benchmark._test_ood_dataset.get_simulations_sizes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result returned by this method contains all the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of an augmented simulator <a id=\"bench1-comp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can assess the performance of the \"augmented simulator\". If we want to retrieve the ML metrics on the test dataset we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_metrics = \"ML\"\n",
    "dataset_name = \"test\"\n",
    "print(\"Fully Connected Augmented Simulator\")\n",
    "print(f\"Dataset : {dataset_name}\")\n",
    "print(\"{:<10} : {}\".format(\"ML metrics\", fc_metrics_test[dataset_name][ML_metrics]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physic compliance\n",
    "A trained augmented simulator could make some errors when verifying physics compliances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physic_compliances = \"Physics\"\n",
    "dataset_name = \"test\"\n",
    "physical_metrics = fc_metrics_test[dataset_name][physic_compliances]\n",
    "print(physical_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOD\n",
    "Assess whether the augmented simulator perform well on data that were not seen during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_ml_metrics = fc_metrics_test_ood[\"test_ood\"][ML_metrics]\n",
    "ood_physical_metrics = fc_metrics_test_ood[\"test_ood\"][physic_compliances]\n",
    "print(ood_ml_metrics)\n",
    "print(ood_physical_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the score\n",
    "\n",
    "In order to focus here on the process rather on the precise details, we skip the explanations regarding the score computation. For more details, we refer to [this notebook](https://github.com/IRT-SystemX/ml4physim_startingkit/blob/main/5_Scoring.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the thresholds and the airfoil configuration (can not be changed by the user and does not depend on the augmented solver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds={\"x-velocity\":(0.05,0.1,\"min\"),\n",
    "            \"y-velocity\":(0.05,0.1,\"min\"),\n",
    "            \"pressure\":(0.05,0.1,\"min\"),\n",
    "            \"pressure_surfacic\":(0.05,0.1,\"min\"),\n",
    "            \"turbulent_viscosity\":(0.05,0.1,\"min\"),\n",
    "            \"mean_relative_drag\":(0.05,0.1,\"min\"),\n",
    "            \"mean_relative_lift\":(0.05,0.1,\"min\"),\n",
    "            \"spearman_correlation_drag\":(0.5,0.8,\"max\"),\n",
    "            \"spearman_correlation_lift\":(0.5,0.8,\"max\")           \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration={\n",
    "        \"coefficients\":{\"ML\":0.4,\"OOD\":0.3,\"Physics\":0.3},\n",
    "        \"ratioRelevance\":{\"Speed-up\":0.25,\"Accuracy\":0.75},\n",
    "        \"valueByColor\":{\"g\":2,\"o\":1,\"r\":0},\n",
    "        \"maxSpeedRatioAllowed\":10000,\n",
    "        \"reference_mean_simulation_time\":1500\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect the appropriate metrics for each variables from the evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_metrics = fc_metrics_test[\"test\"][ML_metrics][\"MAPE\"]\n",
    "ml_metrics[\"pressure_surfacic\"] = fc_metrics_test[\"test\"][ML_metrics][\"MAPE_surfacic\"][\"pressure\"]\n",
    "\n",
    "phy_variables_to_keep = [\"mean_relative_drag\",\"mean_relative_lift\",\"spearman_correlation_drag\",\"spearman_correlation_lift\"]\n",
    "phy_metrics = {phy_variable:fc_metrics_test[\"test\"][physic_compliances][phy_variable] for phy_variable in phy_variables_to_keep}\n",
    "\n",
    "ml_ood_metrics = fc_metrics_test_ood[\"test_ood\"][ML_metrics][\"MAPE\"]\n",
    "ml_ood_metrics[\"pressure_surfacic\"] = fc_metrics_test_ood[\"test_ood\"][ML_metrics][\"MAPE_surfacic\"][\"pressure\"]\n",
    "phy_ood_metrics = {phy_variable:fc_metrics_test_ood[\"test_ood\"][physic_compliances][phy_variable] for phy_variable in phy_variables_to_keep}\n",
    "ood_metrics = {**ml_ood_metrics,**phy_ood_metrics}\n",
    "\n",
    "all_metrics={\n",
    "    ML_metrics:ml_metrics,\n",
    "    physic_compliances:phy_metrics,\n",
    "    \"OOD\":ood_metrics\n",
    "}\n",
    "\n",
    "print(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the speed-up for the test dataset and out of distribution dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_mean_simulation_time=configuration[\"reference_mean_simulation_time\"]\n",
    "speedUp={\n",
    "        ML_metrics:reference_mean_simulation_time/test_mean_simulation_time,\n",
    "         \"OOD\":reference_mean_simulation_time/test_mean_simulation_time\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the function to compute the global score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_global_score(metrics,speedUp,configuration,thresholds):\n",
    "    accuracyResults=dict()\n",
    "    for subcategoryName, subcategoryVal in metrics.items():\n",
    "        accuracyResults[subcategoryName]=[]\n",
    "        for variableName, variableError in subcategoryVal.items():\n",
    "            thresholdMin,thresholdMax,evalType=thresholds[variableName]\n",
    "            if evalType==\"min\":\n",
    "                if variableError<thresholdMin:\n",
    "                    accuracyEval=\"g\"\n",
    "                elif thresholdMin<variableError<thresholdMax:\n",
    "                    accuracyEval=\"o\"\n",
    "                else:\n",
    "                    accuracyEval=\"r\"\n",
    "            elif evalType==\"max\":\n",
    "                if variableError<thresholdMin:\n",
    "                    accuracyEval=\"r\"\n",
    "                elif thresholdMin<variableError<thresholdMax:\n",
    "                    accuracyEval=\"o\"\n",
    "                else:\n",
    "                    accuracyEval=\"g\"\n",
    "    \n",
    "            accuracyResults[subcategoryName].append(accuracyEval)\n",
    "\n",
    "    print(\"accuracyResults: \",accuracyResults)\n",
    "    \n",
    "    coefficients=configuration[\"coefficients\"]\n",
    "    ratioRelevance=configuration[\"ratioRelevance\"]\n",
    "    valueByColor=configuration[\"valueByColor\"]\n",
    "    maxSpeedRatioAllowed=configuration[\"maxSpeedRatioAllowed\"]\n",
    "    mlSubscore=0\n",
    "    accuracyMaxPoints=ratioRelevance[\"Accuracy\"]\n",
    "    accuracyResult=sum([valueByColor[color] for color in accuracyResults[\"ML\"]])\n",
    "    accuracyResult=accuracyResult*accuracyMaxPoints/(len(accuracyResults[\"ML\"])*max(valueByColor.values()))\n",
    "    mlSubscore+=accuracyResult\n",
    "    speedUpMaxPoints=ratioRelevance[\"Speed-up\"]\n",
    "    speedUpResult=max(0,min(math.log10(speedUp[\"ML\"])/math.log10(maxSpeedRatioAllowed),1))\n",
    "    speedUpResult=speedUpResult*speedUpMaxPoints\n",
    "    mlSubscore+=speedUpResult\n",
    "\n",
    "    accuracyResult=sum([valueByColor[color] for color in accuracyResults[\"Physics\"]])\n",
    "    accuracyResult=accuracyResult/(len(accuracyResults[\"Physics\"])*max(valueByColor.values()))\n",
    "    physicsSubscore=accuracyResult\n",
    "\n",
    "    oodSubscore=0\n",
    "    accuracyMaxPoints=ratioRelevance[\"Accuracy\"]\n",
    "    accuracyResult=sum([valueByColor[color] for color in accuracyResults[\"OOD\"]])\n",
    "    accuracyResult=accuracyResult*accuracyMaxPoints/(len(accuracyResults[\"OOD\"])*max(valueByColor.values()))\n",
    "    oodSubscore+=accuracyResult\n",
    "    \n",
    "    speedUpMaxPoints=ratioRelevance[\"Speed-up\"]\n",
    "    speedUpResult=max(0,min(math.log10(speedUp[\"OOD\"])/math.log10(maxSpeedRatioAllowed),1))\n",
    "    speedUpResult=speedUpResult*speedUpMaxPoints\n",
    "    oodSubscore+=speedUpResult\n",
    "\n",
    "    globalScore=100*(coefficients[\"ML\"]*mlSubscore+coefficients[\"Physics\"]*physicsSubscore+coefficients[\"OOD\"]*oodSubscore)\n",
    "    return globalScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we compute the global score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalScore = compute_global_score(all_metrics,speedUp,configuration,thresholds)\n",
    "print(globalScore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
