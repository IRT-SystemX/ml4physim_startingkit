{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86eb33b-8fde-4699-be62-7a2a4d93b04f",
   "metadata": {},
   "source": [
    " # Evaluate Baselines (Airfoil use case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3874807c-36e0-4733-bea8-3707e47b3db5",
   "metadata": {},
   "source": [
    "The goal of this notebook is to demonstrate how we can evaluate the results of a baseline on a given benchmark for a fully custom model with its own training procedure. \n",
    "\n",
    "In this case, LIPS is used mainly for the evaluation with a few exceptions (scaler, rebuilding the dataset from the prediction...)\n",
    "\n",
    "We will show how to run the baseline and evaluate it on the Benchmark of the competition. The methodology is akin to the one described in the AirfRANS paper for a MLP model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3657a914-c7a9-42b2-a11a-f6188685cabd",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063dfd48-5123-4db2-b4f9-87eec92c2dc4",
   "metadata": {},
   "source": [
    "Install the LIPS framework if it is not already done. For more information look at the LIPS framework [Github repository](https://github.com/IRT-SystemX/LIPS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62881df-82af-451c-995b-a4ea1473c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# or \n",
    "# !pip install -U ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ec768-9532-4ff8-8939-69b51dc3e92d",
   "metadata": {},
   "source": [
    "Install the AirfRANS package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc53978-74b3-4335-a607-87fee1868325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install airfrans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33c321-9fbd-4a50-875c-f78645c22cec",
   "metadata": {},
   "source": [
    "#### Import required LIPS packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601955aa-7888-429a-b307-1f1538aaf9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips import get_root_path\n",
    "from lips.benchmark.airfransBenchmark import AirfRANSBenchmark\n",
    "from lips.dataset.airfransDataSet import download_data\n",
    "from lips.dataset.scaler.standard_scaler_iterative import StandardScalerIterative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2e886-80de-4b5f-8ffb-107792f1c708",
   "metadata": {},
   "source": [
    "#### Import other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734e073-205f-4450-afb6-7b65c14f746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math \n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import BatchNorm1d, Identity\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500f7bf1-440c-47d5-8d0c-fd81c10d4181",
   "metadata": {},
   "source": [
    "Define the configuration files path, that aim to describe specific caracteristics of the use case or the augmented simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c1ae6-3409-41e1-8039-7c326b811f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIPS_PATH = get_root_path()\n",
    "DIRECTORY_NAME = 'Dataset'\n",
    "BENCHMARK_NAME = \"DEFAULT\"\n",
    "LOG_PATH = LIPS_PATH + \"lips_logs.log\"\n",
    "BENCH_CONFIG_PATH = os.path.join(\"airfoilConfigurations\",\"benchmarks\",\"confAirfoil.ini\") \n",
    "\n",
    "directory_name='Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95525c32-829f-49da-b296-aa739e6dd6c8",
   "metadata": {},
   "source": [
    "First, we define a simple MLP class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c427fd-63df-4969-8d00-db068b829b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, channel_list, dropout = 0.,\n",
    "                 batch_norm = True, relu_first = False):\n",
    "        super().__init__()\n",
    "        assert len(channel_list) >= 2\n",
    "        self.channel_list = channel_list\n",
    "        self.dropout = dropout\n",
    "        self.relu_first = relu_first\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        for dims in zip(self.channel_list[:-1], self.channel_list[1:]):\n",
    "            self.lins.append(Linear(*dims))\n",
    "\n",
    "        self.norms = torch.nn.ModuleList()\n",
    "        for dim in zip(self.channel_list[1:-1]):\n",
    "            self.norms.append(BatchNorm1d(dim, track_running_stats = False) if batch_norm else Identity())\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        for norm in self.norms:\n",
    "            if hasattr(norm, 'reset_parameters'):\n",
    "                norm.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.lins[0](x)\n",
    "        for lin, norm in zip(self.lins[1:], self.norms):\n",
    "            if self.relu_first:\n",
    "                x = x.relu_()\n",
    "            x = norm(x)\n",
    "            if not self.relu_first:\n",
    "                x = x.relu_()\n",
    "            x = F.dropout(x, p = self.dropout, training = self.training)\n",
    "            x = lin.forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}({str(self.channel_list)[1:-1]})'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ffabf-cbea-4c76-8c13-73ad566bf00b",
   "metadata": {},
   "source": [
    "and we consider the following neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656bb3f-759c-48f6-bc42-aaa1fcec65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, hparams, encoder, decoder):\n",
    "        super(NN, self).__init__()\n",
    "        self.nb_hidden_layers = hparams['nb_hidden_layers']\n",
    "        self.size_hidden_layers = hparams['size_hidden_layers']\n",
    "        self.bn_bool = hparams['bn_bool']\n",
    "        self.activation = nn.ReLU()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.dim_enc = hparams['encoder'][-1]\n",
    "        self.nn = MLP([self.dim_enc] + [self.size_hidden_layers]*self.nb_hidden_layers + [self.dim_enc], batch_norm = self.bn_bool)\n",
    "\n",
    "    def forward(self, data):\n",
    "        z = self.encoder(data.x)        \n",
    "        z = self.nn(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d39434-d83b-4141-9063-ce6bb5a00a44",
   "metadata": {},
   "source": [
    "Now, we are in position to implement the augmented simulator. There are two points worth mentionning in this approach:\n",
    "\n",
    "- The neural network model relies on a representation that allow it to see each CFD simulation within a dataset individually (see `process_dataset` method below for more details)\n",
    "- The training involves a subsampling of a constant number of points within each simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59fafb6-fa5c-4bb0-9d97-cbeb1253a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedSimulator():\n",
    "    def __init__(self,benchmark,**kwargs):\n",
    "        self.name = \"AirfRANSSubmission\"\n",
    "        chunk_sizes=benchmark.train_dataset.get_simulations_sizes()\n",
    "        scalerParams={\"chunk_sizes\":chunk_sizes}\n",
    "        self.scaler = StandardScalerIterative(**scalerParams)\n",
    "        self.hparams = kwargs\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        self.device = 'cuda:0' if use_cuda else 'cpu'\n",
    "        if use_cuda:\n",
    "            print('Using GPU')\n",
    "        else:\n",
    "            print('Using CPU')\n",
    "\n",
    "        encoder = MLP(self.hparams['encoder'], batch_norm = False)\n",
    "        decoder = MLP(self.hparams['decoder'], batch_norm = False)\n",
    "        self.model = NN(self.hparams, encoder, decoder)\n",
    "\n",
    "    def train(self,train_dataset):\n",
    "        train_dataset = self.process_dataset(dataset=train_dataset,training=True)\n",
    "        global_train(self.device, train_dataset, self.model, self.hparams,criterion = 'MSE_weighted')\n",
    "\n",
    "    def predict(self,dataset,**kwargs):\n",
    "        print(dataset)\n",
    "        test_dataset = self.process_dataset(dataset=dataset,training=False)\n",
    "        self.model.eval()\n",
    "        avg_loss_per_var = np.zeros(4)\n",
    "        avg_loss = 0\n",
    "        avg_loss_surf_var = np.zeros(4)\n",
    "        avg_loss_vol_var = np.zeros(4)\n",
    "        avg_loss_surf = 0\n",
    "        avg_loss_vol = 0\n",
    "        iterNum = 0\n",
    "\n",
    "        predictions=[]\n",
    "        with torch.no_grad():\n",
    "            for data in test_dataset:        \n",
    "                data_clone = data.clone()\n",
    "                data_clone = data_clone.to(self.device)\n",
    "                out = self.model(data_clone)\n",
    "\n",
    "                targets = data_clone.y\n",
    "                loss_criterion = nn.MSELoss(reduction = 'none')\n",
    "\n",
    "                loss_per_var = loss_criterion(out, targets).mean(dim = 0)\n",
    "                loss = loss_per_var.mean()\n",
    "                loss_surf_var = loss_criterion(out[data_clone.surf, :], targets[data_clone.surf, :]).mean(dim = 0)\n",
    "                loss_vol_var = loss_criterion(out[~data_clone.surf, :], targets[~data_clone.surf, :]).mean(dim = 0)\n",
    "                loss_surf = loss_surf_var.mean()\n",
    "                loss_vol = loss_vol_var.mean()  \n",
    "\n",
    "                avg_loss_per_var += loss_per_var.cpu().numpy()\n",
    "                avg_loss += loss.cpu().numpy()\n",
    "                avg_loss_surf_var += loss_surf_var.cpu().numpy()\n",
    "                avg_loss_vol_var += loss_vol_var.cpu().numpy()\n",
    "                avg_loss_surf += loss_surf.cpu().numpy()\n",
    "                avg_loss_vol += loss_vol.cpu().numpy()  \n",
    "                iterNum += 1\n",
    "\n",
    "                out = out.cpu().data.numpy()\n",
    "                prediction = self._post_process(out)\n",
    "                predictions.append(prediction)\n",
    "        print(\"Results for test\")\n",
    "        print(avg_loss/iterNum, avg_loss_per_var/iterNum, avg_loss_surf_var/iterNum, avg_loss_vol_var/iterNum, avg_loss_surf/iterNum, avg_loss_vol/iterNum)\n",
    "        predictions= np.vstack(predictions)\n",
    "        predictions = dataset.reconstruct_output(predictions)\n",
    "        return predictions\n",
    "\n",
    "    def save_model(self, path_model:str,path_scaler:str):\n",
    "        modelWeight=self.model.state_dict()\n",
    "        torch.save(modelWeight,path_model)\n",
    "        self.scaler.save(path_scaler)\n",
    "\n",
    "    def load_model(self, path_model:str,path_scaler:str):\n",
    "        model_loader=torch.load(path_model)\n",
    "        self.model.load_state_dict(model_loader)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.scaler.load(path_scaler)\n",
    "\n",
    "    def process_dataset(self, dataset, training: bool) -> DataLoader:\n",
    "        coord_x=dataset.data['x-position']\n",
    "        coord_y=dataset.data['y-position']\n",
    "        surf_bool=dataset.extra_data['surface']\n",
    "        position = np.stack([coord_x,coord_y],axis=1)\n",
    "\n",
    "        nodes_features,node_labels=dataset.extract_data()\n",
    "        if training:\n",
    "            print(\"Normalize train data\")\n",
    "            nodes_features, node_labels = self.scaler.fit_transform(nodes_features, node_labels)\n",
    "        else:\n",
    "            print(\"Normalize not train data\")\n",
    "            nodes_features, node_labels = self.scaler.transform(nodes_features, node_labels)\n",
    "\n",
    "        torchDataset=[]\n",
    "        nb_nodes_in_simulations = dataset.get_simulations_sizes()\n",
    "        start_index = 0\n",
    "        for nb_nodes_in_simulation in nb_nodes_in_simulations:\n",
    "            end_index = start_index+nb_nodes_in_simulation\n",
    "            simulation_positions = torch.tensor(position[start_index:end_index,:], dtype = torch.float) \n",
    "            simulation_features = torch.tensor(nodes_features[start_index:end_index,:], dtype = torch.float) \n",
    "            simulation_labels = torch.tensor(node_labels[start_index:end_index,:], dtype = torch.float) \n",
    "            simulation_surface = torch.tensor(surf_bool[start_index:end_index])\n",
    "\n",
    "            sampleData=Data(pos=simulation_positions,\n",
    "                            x=simulation_features, \n",
    "                            y=simulation_labels,\n",
    "                            surf = simulation_surface.bool()) \n",
    "            torchDataset.append(sampleData)\n",
    "            start_index += nb_nodes_in_simulation\n",
    "        return DataLoader(dataset=torchDataset,batch_size=1)\n",
    "\n",
    "    \n",
    "    def _post_process(self, data):\n",
    "        try:\n",
    "            processed = self.scaler.inverse_transform(data)\n",
    "        except TypeError:\n",
    "            processed = self.scaler.inverse_transform(data.cpu())\n",
    "        return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8683fd-6872-4e33-aa04-22b50495f519",
   "metadata": {},
   "source": [
    "Where the global train function is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209b87b-a1d9-497b-b19b-bbdef57719cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_train(device, train_dataset, network, hparams, criterion = 'MSE', reg = 1):\n",
    "    model = network.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = hparams['lr'])\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr = hparams['lr'],\n",
    "            total_steps = (len(train_dataset) // hparams['batch_size'] + 1) * hparams['nb_epochs'],\n",
    "        )\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss_surf_list = []\n",
    "    train_loss_vol_list = []\n",
    "    loss_surf_var_list = []\n",
    "    loss_vol_var_list = []\n",
    "\n",
    "    pbar_train = tqdm(range(hparams['nb_epochs']), position=0)\n",
    "    for epoch in pbar_train:        \n",
    "        train_dataset_sampled = []\n",
    "        for data in train_dataset:\n",
    "            data_sampled = data.clone()\n",
    "            idx = random.sample(range(data_sampled.x.size(0)), hparams['subsampling'])\n",
    "            idx = torch.tensor(idx)\n",
    "\n",
    "            data_sampled.pos = data_sampled.pos[idx]\n",
    "            data_sampled.x = data_sampled.x[idx]\n",
    "            data_sampled.y = data_sampled.y[idx]\n",
    "            data_sampled.surf = data_sampled.surf[idx]\n",
    "            train_dataset_sampled.append(data_sampled)\n",
    "        train_loader = DataLoader(train_dataset_sampled, batch_size = hparams['batch_size'], shuffle = True)\n",
    "        del(train_dataset_sampled)\n",
    "\n",
    "        train_loss, _, loss_surf_var, loss_vol_var, loss_surf, loss_vol = train_model(device, model, train_loader, optimizer, lr_scheduler, criterion, reg = reg)        \n",
    "        if criterion == 'MSE_weighted':\n",
    "            train_loss = reg*loss_surf + loss_vol\n",
    "        del(train_loader)\n",
    "\n",
    "        train_loss_surf_list.append(loss_surf)\n",
    "        train_loss_vol_list.append(loss_vol)\n",
    "        loss_surf_var_list.append(loss_surf_var)\n",
    "        loss_vol_var_list.append(loss_vol_var)\n",
    "\n",
    "    loss_surf_var_list = np.array(loss_surf_var_list)\n",
    "    loss_vol_var_list = np.array(loss_vol_var_list)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(device, model, train_loader, optimizer, scheduler, criterion = 'MSE', reg = 1):\n",
    "    model.train()\n",
    "    avg_loss_per_var = torch.zeros(4, device = device)\n",
    "    avg_loss = 0\n",
    "    avg_loss_surf_var = torch.zeros(4, device = device)\n",
    "    avg_loss_vol_var = torch.zeros(4, device = device)\n",
    "    avg_loss_surf = 0\n",
    "    avg_loss_vol = 0\n",
    "    iterNum = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        data_clone = data.clone()\n",
    "        data_clone = data_clone.to(device)   \n",
    "        optimizer.zero_grad()  \n",
    "        out = model(data_clone)\n",
    "        targets = data_clone.y\n",
    "\n",
    "        if criterion == 'MSE' or criterion == 'MSE_weighted':\n",
    "            loss_criterion = nn.MSELoss(reduction = 'none')\n",
    "        elif criterion == 'MAE':\n",
    "            loss_criterion = nn.L1Loss(reduction = 'none')\n",
    "        loss_per_var = loss_criterion(out, targets).mean(dim = 0)\n",
    "        total_loss = loss_per_var.mean()\n",
    "        loss_surf_var = loss_criterion(out[data_clone.surf, :], targets[data_clone.surf, :]).mean(dim = 0)\n",
    "        loss_vol_var = loss_criterion(out[~data_clone.surf, :], targets[~data_clone.surf, :]).mean(dim = 0)\n",
    "        loss_surf = loss_surf_var.mean()\n",
    "        loss_vol = loss_vol_var.mean()\n",
    "\n",
    "        if criterion == 'MSE_weighted':            \n",
    "            (loss_vol + reg*loss_surf).backward()           \n",
    "        else:\n",
    "            total_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        avg_loss_per_var += loss_per_var\n",
    "        avg_loss += total_loss\n",
    "        avg_loss_surf_var += loss_surf_var\n",
    "        avg_loss_vol_var += loss_vol_var\n",
    "        avg_loss_surf += loss_surf\n",
    "        avg_loss_vol += loss_vol \n",
    "        iterNum += 1\n",
    "\n",
    "    return avg_loss.cpu().data.numpy()/iterNum, avg_loss_per_var.cpu().data.numpy()/iterNum, avg_loss_surf_var.cpu().data.numpy()/iterNum, avg_loss_vol_var.cpu().data.numpy()/iterNum, \\\n",
    "            avg_loss_surf.cpu().data.numpy()/iterNum, avg_loss_vol.cpu().data.numpy()/iterNum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b1cd5e-396f-46f1-8486-dcd49b72a847",
   "metadata": {},
   "source": [
    "## Initial step: download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feda344e-59f3-4e32-88e1-f32aa7d83346",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(directory_name):\n",
    "    download_data(root_path=\".\", directory_name=directory_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb3542c-b354-4f4d-9b5b-3db8d204e8ce",
   "metadata": {},
   "source": [
    "#  Benchmark <a id=\"Case1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b940d8-bc5f-4851-9f65-4dfeacc42e86",
   "metadata": {},
   "source": [
    "## Loading the datasets\n",
    "\n",
    "A common dataset will be used for evaluate the augmented simulator. This initial step aims at loading it once and for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac38eb0-a66e-4f3c-a717-f521728ed44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark=AirfRANSBenchmark(benchmark_path = DIRECTORY_NAME,\n",
    "                            config_path = BENCH_CONFIG_PATH,\n",
    "                            benchmark_name = BENCHMARK_NAME,\n",
    "                            log_path = LOG_PATH)\n",
    "benchmark.load(path=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f3ecd3-66eb-4a14-8c00-1ccb7fe0fd80",
   "metadata": {},
   "source": [
    "## A baseline \"augmented simulator\" <a id=\"bench1-fc\"></a>\n",
    "\n",
    "Along with some dataset, we provide also a baseline (from a trained neural network) with several hyperparameters that can be calibrated accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34603f3e-820a-47e5-a3ae-00af1f7a0799",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"encoder\": [7, 64, 64, 8],\n",
    "               \"decoder\": [8, 64, 64, 4],\n",
    "               \"nb_hidden_layers\": 3,\n",
    "               \"size_hidden_layers\": 64,\n",
    "               \"batch_size\": 1,\n",
    "               \"nb_epochs\": 600,\n",
    "               \"lr\": 0.001,\n",
    "               \"bn_bool\": True,\n",
    "               \"subsampling\": 32000}\n",
    "\n",
    "mySimulator = AugmentedSimulator(benchmark=benchmark,**parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badcba6f-5c3f-4b47-87fc-25c5d972d22f",
   "metadata": {},
   "source": [
    "Training the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419cee51-a6dc-42fd-8dd4-c3be8c6c247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mySimulator.train(benchmark.train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9ab46-84a1-4564-adbe-8597ab1f320d",
   "metadata": {},
   "source": [
    "It is also possible to save the model weights/scaler normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478bf35-7068-4eab-bebb-fae53edb6e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "mySimulator.save_model(\"SaveFCModel.pt\",\"SaveScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04971399-f5ce-42b8-b8fd-f317b6a7cd8f",
   "metadata": {},
   "source": [
    "We are now in position to reload the model/scaler without training it once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f6b21-6d0e-4fa1-b25b-ede85b041850",
   "metadata": {},
   "outputs": [],
   "source": [
    "mySimulator2 = AugmentedSimulator(benchmark=benchmark,**parameters)\n",
    "mySimulator2.load_model(\"SaveFCModel.pt\",\"SaveScaler\")\n",
    "prediction_from_reloaded_model = mySimulator2.predict(benchmark._test_dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f0177d2-df1e-4898-a9f3-ee3eb91e7ba5",
   "metadata": {},
   "source": [
    "The prediction should be the same for the original model and the reloaded one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d6f277-106a-4b9c-94b5-8cb77afec046",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_original = mySimulator.predict(benchmark._test_dataset)\n",
    "\n",
    "for variable in prediction_original.keys():\n",
    "    np.testing.assert_almost_equal(prediction_original[variable],prediction_from_reloaded_model[variable],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f02ee9b-71d8-4561-8586-45ceb853e5a1",
   "metadata": {},
   "source": [
    "Evaluating the baseline on the test and out-of-distribution dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33de6d0-92a5-4034-bfb0-41549f2435ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_test = time.time()\n",
    "fc_metrics_test = benchmark.evaluate_simulator(dataset=\"test\",augmented_simulator=mySimulator)\n",
    "test_evaluation_time = time.time() - start_test\n",
    "test_mean_simulation_time = test_evaluation_time/len(benchmark._test_dataset.get_simulations_sizes())\n",
    "print(\"Test evaluation time:\",test_evaluation_time)\n",
    "\n",
    "print(\"Fully Connected Augmented Simulator\")\n",
    "ml_metrics = fc_metrics_test[\"test\"][\"ML\"]\n",
    "print(\"{:<10} : {}\".format(\"Test ML metrics\", ml_metrics))\n",
    "physical_metrics = fc_metrics_test[\"test\"][\"Physics\"]\n",
    "print(\"{:<10} : {}\".format(\"Test Physical metrics\", physical_metrics))\n",
    "\n",
    "start_test_ood = time.time()\n",
    "fc_metrics_test_ood = benchmark.evaluate_simulator(dataset=\"test_ood\",augmented_simulator=mySimulator)\n",
    "test_ood_evaluation_time = time.time() - start_test_ood\n",
    "test_ood_mean_simulation_time = test_ood_evaluation_time/len(benchmark._test_ood_dataset.get_simulations_sizes())\n",
    "print(\"Test ood evaluation time:\",test_ood_evaluation_time)\n",
    "\n",
    "ood_ml_metrics = fc_metrics_test_ood[\"test_ood\"][\"ML\"]\n",
    "print(\"{:<10} : {}\".format(\"OOD ML metrics\", ood_ml_metrics))\n",
    "ood_physical_metrics = fc_metrics_test_ood[\"test_ood\"][\"Physics\"]\n",
    "print(\"{:<10} : {}\".format(\"OOD physical metrics\", ood_physical_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30932f8e-2d88-4009-8168-49cafada1f9f",
   "metadata": {},
   "source": [
    "## Compute the score\n",
    "\n",
    "In order to focus here on the process rather on the precise details, we skip the explanations regarding the score computation. For more details, we refer to [this notebook](https://github.com/IRT-SystemX/ml4physim_startingkit/blob/main/5_Scoring.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea9934f-4af7-4501-969f-b34d4da7d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_global_score(test_result,ood_result,test_mean_simulation_time,test_ood_mean_simulation_time):\n",
    "    thresholds={\"x-velocity\":(0.1,0.2,\"min\"),\n",
    "            \"y-velocity\":(0.1,0.2,\"min\"),\n",
    "            \"pressure\":(0.02,0.1,\"min\"),\n",
    "            \"pressure_surfacic\":(0.08,0.2,\"min\"),\n",
    "            \"turbulent_viscosity\":(0.5,1.0,\"min\"),\n",
    "            \"mean_relative_drag\":(1.0,10.0,\"min\"),\n",
    "            \"mean_relative_lift\":(0.2,0.5,\"min\"),\n",
    "            \"spearman_correlation_drag\":(0.5,0.8,\"max\"),\n",
    "            \"spearman_correlation_lift\":(0.94,0.98,\"max\")           \n",
    "    }\n",
    "    configuration={\n",
    "        \"coefficients\":{\"ML\":0.4,\"OOD\":0.3,\"Physics\":0.3},\n",
    "        \"ratioRelevance\":{\"Speed-up\":0.25,\"Accuracy\":0.75},\n",
    "        \"valueByColor\":{\"g\":2,\"o\":1,\"r\":0},\n",
    "        \"maxSpeedRatioAllowed\":10000,\n",
    "        \"reference_mean_simulation_time\":1500\n",
    "    }\n",
    "\n",
    "    ml_metrics = test_result[\"test\"][\"ML\"][\"MSE_normalized\"]\n",
    "    ml_metrics[\"pressure_surfacic\"] = test_result[\"test\"][\"ML\"][\"MSE_normalized_surfacic\"][\"pressure\"]\n",
    "\n",
    "    phy_variables_to_keep = [\"mean_relative_drag\",\"mean_relative_lift\",\"spearman_correlation_drag\",\"spearman_correlation_lift\"]\n",
    "    phy_metrics = {phy_variable:test_result[\"test\"][\"Physics\"][phy_variable] for phy_variable in phy_variables_to_keep}\n",
    "\n",
    "    ml_ood_metrics = ood_result[\"test_ood\"][\"ML\"][\"MSE_normalized\"]\n",
    "    ml_ood_metrics[\"pressure_surfacic\"] = ood_result[\"test_ood\"][\"ML\"][\"MSE_normalized_surfacic\"][\"pressure\"]\n",
    "    phy_ood_metrics = {phy_variable:ood_result[\"test_ood\"][\"Physics\"][phy_variable] for phy_variable in phy_variables_to_keep}\n",
    "    ood_metrics = {**ml_ood_metrics,**phy_ood_metrics}\n",
    "\n",
    "    all_metrics={\n",
    "        \"ML\":ml_metrics,\n",
    "        \"Physics\":phy_metrics,\n",
    "        \"OOD\":ood_metrics\n",
    "    }\n",
    "\n",
    "    print(all_metrics)\n",
    "\n",
    "    reference_mean_simulation_time=configuration[\"reference_mean_simulation_time\"]\n",
    "    speedUp={\n",
    "        \"ML\":reference_mean_simulation_time/test_mean_simulation_time,\n",
    "         \"OOD\":reference_mean_simulation_time/test_ood_mean_simulation_time\n",
    "        }\n",
    "\n",
    "    accuracyResults=dict()\n",
    "    for subcategoryName, subcategoryVal in all_metrics.items():\n",
    "        accuracyResults[subcategoryName]=[]\n",
    "        for variableName, variableError in subcategoryVal.items():\n",
    "            thresholdMin,thresholdMax,evalType=thresholds[variableName]\n",
    "            if evalType==\"min\":\n",
    "                if variableError<thresholdMin:\n",
    "                    accuracyEval=\"g\"\n",
    "                elif thresholdMin<variableError<thresholdMax:\n",
    "                    accuracyEval=\"o\"\n",
    "                else:\n",
    "                    accuracyEval=\"r\"\n",
    "            elif evalType==\"max\":\n",
    "                if variableError<thresholdMin:\n",
    "                    accuracyEval=\"r\"\n",
    "                elif thresholdMin<variableError<thresholdMax:\n",
    "                    accuracyEval=\"o\"\n",
    "                else:\n",
    "                    accuracyEval=\"g\"\n",
    "    \n",
    "            accuracyResults[subcategoryName].append(accuracyEval)\n",
    "\n",
    "    print(\"accuracyResults: \",accuracyResults)\n",
    "    \n",
    "    coefficients=configuration[\"coefficients\"]\n",
    "    ratioRelevance=configuration[\"ratioRelevance\"]\n",
    "    valueByColor=configuration[\"valueByColor\"]\n",
    "    maxSpeedRatioAllowed=configuration[\"maxSpeedRatioAllowed\"]\n",
    "    mlSubscore=0\n",
    "    accuracyMaxPoints=ratioRelevance[\"Accuracy\"]\n",
    "    accuracyResult=sum([valueByColor[color] for color in accuracyResults[\"ML\"]])\n",
    "    accuracyResult=accuracyResult*accuracyMaxPoints/(len(accuracyResults[\"ML\"])*max(valueByColor.values()))\n",
    "    mlSubscore+=accuracyResult\n",
    "    print(\"ML accuracyResult\",accuracyResult)\n",
    "\n",
    "    speedUpMaxPoints=ratioRelevance[\"Speed-up\"]\n",
    "    speedUpResult=max(0,min(math.log10(speedUp[\"ML\"])/math.log10(maxSpeedRatioAllowed),1))\n",
    "    speedUpResult=speedUpResult*speedUpMaxPoints\n",
    "    mlSubscore+=speedUpResult\n",
    "    print(\"ML speedUpResult\",speedUpResult)\n",
    "\n",
    "    accuracyResult=sum([valueByColor[color] for color in accuracyResults[\"Physics\"]])\n",
    "    accuracyResult=accuracyResult/(len(accuracyResults[\"Physics\"])*max(valueByColor.values()))\n",
    "    physicsSubscore=accuracyResult\n",
    "    print(\"Phy accuracyResult\",accuracyResult)\n",
    "\n",
    "    oodSubscore=0\n",
    "    accuracyMaxPoints=ratioRelevance[\"Accuracy\"]\n",
    "    accuracyResult=sum([valueByColor[color] for color in accuracyResults[\"OOD\"]])\n",
    "    accuracyResult=accuracyResult*accuracyMaxPoints/(len(accuracyResults[\"OOD\"])*max(valueByColor.values()))\n",
    "    oodSubscore+=accuracyResult\n",
    "    print(\"OOD accuracyResult\",accuracyResult)\n",
    "\n",
    "    speedUpMaxPoints=ratioRelevance[\"Speed-up\"]\n",
    "    speedUpResult=max(0,min(math.log10(speedUp[\"OOD\"])/math.log10(maxSpeedRatioAllowed),1))\n",
    "    speedUpResult=speedUpResult*speedUpMaxPoints\n",
    "    oodSubscore+=speedUpResult\n",
    "    print(\"OOD speedUpResult\",speedUpResult)\n",
    "\n",
    "    globalScore=100*(coefficients[\"ML\"]*mlSubscore+coefficients[\"Physics\"]*physicsSubscore+coefficients[\"OOD\"]*oodSubscore)\n",
    "    return globalScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112dde40-9e41-4348-ba6d-0159b49344a9",
   "metadata": {},
   "source": [
    "Finally, we compute the global score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4155140-704a-48fc-9211-a450eafb5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = compute_global_score(fc_metrics_test,fc_metrics_test_ood,test_mean_simulation_time,test_ood_mean_simulation_time)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
