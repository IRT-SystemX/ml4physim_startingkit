{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the global score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to demonstrate how to compute the global score to evaluate the performance for a given `AugmentedSimulator`. From now on, we assume all the inputs to compute the score are already available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input results description\n",
    "\n",
    "The result provided below are made-up in order to explain how the score is computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmetrics={\"ML\":{\n",
    "                 \"x-velocity\":0.03,\n",
    "                 \"y-velocity\":0.03,\n",
    "                 \"pressure\":0.01,\n",
    "                 \"turbulent_viscosity\":0.08,\n",
    "                 \"pressure_surfacic\":0.27\n",
    "                 },\n",
    "            \"Physics\": \n",
    "                 {\"spearman_correlation_drag\":0.2,\n",
    "                  \"spearman_correlation_lift\":0.6,\n",
    "                  \"mean_relative_drag\":0.18,\n",
    "                  \"mean_relative_lift\":0.25,\n",
    "                  },\n",
    "            \"OOD\": \n",
    "                 {\n",
    "                  \"x-velocity\":0.08,\n",
    "                  \"y-velocity\":0.07,\n",
    "                  \"pressure\":0.055,\n",
    "                  \"turbulent_viscosity\":0.06,\n",
    "                  \"pressure_surfacic\":0.45,\n",
    "                  \"spearman_correlation_drag\":0.1,\n",
    "                  \"spearman_correlation_lift\":0.6,\n",
    "                  \"mean_relative_drag\":0.28,\n",
    "                  \"mean_relative_lift\":0.35,\n",
    "                  }\n",
    "           }\n",
    "\n",
    "speedUp= {\"ML\":1300,\"OOD\":1300}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the acceptability thresholds. \n",
    "Each variable is associated with 2 thresholds used to determine whether the result are great, acceptable or unacceptable and whether the result should be maximized or minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds={\"x-velocity\":(0.05,0.1,\"min\"),\n",
    "            \"y-velocity\":(0.05,0.1,\"min\"),\n",
    "            \"pressure\":(0.05,0.1,\"min\"),\n",
    "            \"pressure_surfacic\":(0.05,0.1,\"min\"),\n",
    "            \"turbulent_viscosity\":(0.05,0.1,\"min\"),\n",
    "            \"mean_relative_drag\":(0.05,0.1,\"min\"),\n",
    "            \"mean_relative_lift\":(0.05,0.1,\"min\"),\n",
    "            \"spearman_correlation_drag\":(0.5,0.8,\"max\"),\n",
    "            \"spearman_correlation_lift\":(0.5,0.8,\"max\")           \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, regarding the value obtained for the variable 'x-velocity'\n",
    "\n",
    "* if it is lower than 5%, the result is great\n",
    "* if it is greater than 5% but lower than 10%, the result is acceptable\n",
    "* if it is greater than 10%, the result is not acceptable\n",
    "\n",
    "However, for the physical criteria 'spearman_correlation_drag':\n",
    "\n",
    "* if its value is lower than 0.5, the result is not acceptable\n",
    "* if it is greater than 0.5 but lower than 0.8, the result is acceptable\n",
    "* if it is greater than 0.8, the result is great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration={\n",
    "        \"coefficients\":{\"ML\":0.4,\"OOD\":0.3,\"Physics\":0.3},\n",
    "        \"ratioRelevance\":{\"Speed-up\":0.25,\"Accuracy\":0.75},\n",
    "        \"valueByColor\":{\"g\":2,\"o\":1,\"r\":0},\n",
    "        \"maxSpeedRatioAllowed\":10000\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the result accuracy performances for all variables. We denote by:\n",
    "\n",
    "* g, a great result\n",
    "* o, an acceptable result\n",
    "* r, a not acceptable result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyResults=dict()\n",
    "for subcategoryName, subcategoryVal in allmetrics.items():\n",
    "    accuracyResults[subcategoryName]=[]\n",
    "    for variableName, variableError in subcategoryVal.items():\n",
    "        thresholdMin,thresholdMax,evalType=thresholds[variableName]\n",
    "        if evalType==\"min\":\n",
    "            if variableError<thresholdMin:\n",
    "                accuracyEval=\"g\"\n",
    "            elif thresholdMin<variableError<thresholdMax:\n",
    "                accuracyEval=\"o\"\n",
    "            else:\n",
    "                accuracyEval=\"r\"\n",
    "        elif evalType==\"max\":\n",
    "            if variableError<thresholdMin:\n",
    "                accuracyEval=\"r\"\n",
    "            elif thresholdMin<variableError<thresholdMax:\n",
    "                accuracyEval=\"o\"\n",
    "            else:\n",
    "                accuracyEval=\"g\"\n",
    "\n",
    "        accuracyResults[subcategoryName].append(accuracyEval)\n",
    "    \n",
    "print(accuracyResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in position to compute each subscore, with minor prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpeedMetric(speedUp,speedMax):\n",
    "    return max(min(math.log10(speedUp)/math.log10(speedMax),1),0)\n",
    "\n",
    "coefficients=configuration[\"coefficients\"]\n",
    "ratioRelevance=configuration[\"ratioRelevance\"]\n",
    "valueByColor=configuration[\"valueByColor\"]\n",
    "maxSpeedRatioAllowed=configuration[\"maxSpeedRatioAllowed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ML subscore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlSubscore=0\n",
    "\n",
    "#Compute accuracy\n",
    "accuracyMaxPoints=ratioRelevance[\"Accuracy\"]\n",
    "accuracyResult=sum([valueByColor[color] for color in accuracyResults[\"ML\"]])\n",
    "accuracyResult=accuracyResult*accuracyMaxPoints/(len(accuracyResults[\"ML\"])*max(valueByColor.values()))\n",
    "mlSubscore+=accuracyResult\n",
    "\n",
    "#Compute speed-up\n",
    "speedUpMaxPoints=ratioRelevance[\"Speed-up\"]\n",
    "speedUpResult=SpeedMetric(speedUp=speedUp[\"ML\"],speedMax=maxSpeedRatioAllowed)\n",
    "speedUpResult=speedUpResult*speedUpMaxPoints\n",
    "mlSubscore+=speedUpResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Physics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute accuracy\n",
    "accuracyResult=sum([valueByColor[color] for color in accuracyResults[\"Physics\"]])\n",
    "accuracyResult=accuracyResult/(len(accuracyResults[\"Physics\"])*max(valueByColor.values()))\n",
    "physicsSubscore=accuracyResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oodSubscore=0\n",
    "\n",
    "#Compute accuracy\n",
    "accuracyMaxPoints=ratioRelevance[\"Accuracy\"]\n",
    "accuracyResult=sum([valueByColor[color] for color in accuracyResults[\"OOD\"]])\n",
    "accuracyResult=accuracyResult*accuracyMaxPoints/(len(accuracyResults[\"OOD\"])*max(valueByColor.values()))\n",
    "oodSubscore+=accuracyResult\n",
    "\n",
    "#Compute speed-up\n",
    "speedUpMaxPoints=ratioRelevance[\"Speed-up\"]\n",
    "speedUpResult=SpeedMetric(speedUp=speedUp[\"OOD\"],speedMax=maxSpeedRatioAllowed)\n",
    "speedUpResult=speedUpResult*speedUpMaxPoints\n",
    "oodSubscore+=speedUpResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalScore=100*(coefficients[\"ML\"]*mlSubscore+coefficients[\"Physics\"]*physicsSubscore+coefficients[\"OOD\"]*oodSubscore)\n",
    "print(globalScore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
